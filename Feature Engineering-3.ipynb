{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb03b14-33bf-4fb8-81e0-380c673ae217",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "Min-Max scaling is a normalization technique used to rescale data to a fixed range, typically between 0 and 1. It transforms features to lie within a specific range by using the following formula:\n",
    "Xscaled=X−XminXmax−XminX_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}Xscaled=Xmax−XminX−Xmin\n",
    "•\tUsage in preprocessing: It’s often used when the data involves features of different scales (e.g., age, income) that could disproportionately affect the model.\n",
    "Example: Suppose we have the following values for a feature: [2,8,10,15][2, 8, 10, 15][2,8,10,15]. The min value is 2 and the max value is 15. Using Min-Max scaling:\n",
    "•\tScaled value for 2: 2−215−2=0\\frac{2 - 2}{15 - 2} = 015−22−2=0\n",
    "•\tScaled value for 8: 8−215−2=0.4615\\frac{8 - 2}{15 - 2} = 0.461515−28−2=0.4615\n",
    "•\tScaled value for 10: 10−215−2=0.6154\\frac{10 - 2}{15 - 2} = 0.615415−210−2=0.6154\n",
    "•\tScaled value for 15: 15−215−2=1\\frac{15 - 2}{15 - 2} = 115−215−2=1\n",
    "The scaled values are now between 0 and 1: [0,0.4615,0.6154,1][0, 0.4615, 0.6154, 1][0,0.4615,0.6154,1].\n",
    "________________________________________\n",
    "Q2: What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "The Unit Vector technique, also known as Normalization, rescales the data so that the magnitude (or length) of the feature vector is 1. This technique ensures that each data point lies on a unit hypersphere.\n",
    "•\tFormula:\n",
    "Xnormalized=X∣∣X∣∣X_{\\text{normalized}} = \\frac{X}{||X||}Xnormalized=∣∣X∣∣X\n",
    "where ∣∣X∣∣||X||∣∣X∣∣ is the Euclidean norm (magnitude) of the feature vector.\n",
    "•\tDifference from Min-Max Scaling: Min-Max scaling rescales each feature independently to a specific range, while Unit Vector normalization ensures that the entire feature vector has a length of 1.\n",
    "Example: Given a feature vector [3,4][3, 4][3,4], the magnitude ∣∣X∣∣||X||∣∣X∣∣ is:\n",
    "∣∣X∣∣=32+42=5||X|| = \\sqrt{3^2 + 4^2} = 5∣∣X∣∣=32+42=5\n",
    "The normalized vector is:\n",
    "(35,45)=(0.6,0.8)\\left(\\frac{3}{5}, \\frac{4}{5}\\right) = (0.6, 0.8)(53,54)=(0.6,0.8)\n",
    "The total magnitude of the scaled vector is now 1.\n",
    "________________________________________\n",
    "Q3: What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. PCA transforms the data into a set of new uncorrelated variables called principal components, which capture the maximum variance in the data.\n",
    "•\tUsage: It reduces the number of features while retaining most of the data’s variance, which helps in speeding up model training and avoiding overfitting.\n",
    "Example: Suppose you have a dataset with 3 features: height, weight, and age. After applying PCA, you might find that height and weight are highly correlated and can be summarized into a single principal component, reducing the dataset from 3 dimensions to 2 while maintaining most of the variance.\n",
    "________________________________________\n",
    "Q4: What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "Feature Extraction involves creating new features by combining or transforming the original ones. PCA is a feature extraction technique because it creates principal components—linear combinations of the original features—that capture the most variance in the data.\n",
    "•\tHow PCA is used for Feature Extraction: PCA transforms the dataset into a new set of features (principal components) where each component explains a portion of the variance. The components can be selected based on how much variance they explain.\n",
    "Example: Consider a dataset with 5 features, and after applying PCA, you obtain 5 principal components. If the first 2 components explain 95% of the variance, you can reduce the dataset to 2 dimensions by selecting only these two components, thereby extracting new features that summarize the dataset.\n",
    "________________________________________\n",
    "Q5: You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "To preprocess the dataset for the food delivery recommendation system using Min-Max scaling:\n",
    "1.\tIdentify the features: Let’s assume the dataset contains numerical features like price, rating, and delivery time.\n",
    "2.\tApply Min-Max scaling:\n",
    "o\tFor each feature, compute the minimum and maximum values.\n",
    "o\tRescale each value using the Min-Max formula: Xscaled=X−XminXmax−XminX_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}Xscaled=Xmax−XminX−Xmin\n",
    "o\tThis ensures that all features (e.g., price, rating, and delivery time) are rescaled to lie within the same range (e.g., 0 to 1), which helps avoid bias during model training.\n",
    "This approach ensures that features on different scales (e.g., delivery time in minutes vs. price in dollars) are normalized, making them equally important in the recommendation system.\n",
    "________________________________________\n",
    "Q6: You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "To reduce the dimensionality of the stock price prediction dataset using PCA:\n",
    "1.\tStandardize the data: Since PCA is sensitive to the scale of the features, standardize the dataset so that each feature has zero mean and unit variance.\n",
    "2.\tCompute the covariance matrix: PCA computes the covariance matrix of the features to understand the variance and correlation between them.\n",
    "3.\tCalculate eigenvalues and eigenvectors: PCA finds the principal components by calculating eigenvectors and eigenvalues of the covariance matrix.\n",
    "4.\tChoose the number of components: Sort the eigenvalues and select the top kkk components that explain most of the variance (e.g., 95%). This reduces the dimensionality while preserving most of the information.\n",
    "5.\tTransform the data: Project the original data onto the principal components to create a new dataset with reduced dimensions.\n",
    "This helps simplify the model, speeds up training, and reduces the risk of overfitting by using fewer features that still capture the key patterns in stock prices.\n",
    "________________________________________\n",
    "Q7: For a dataset containing the following values: [1,5,10,15,20][1, 5, 10, 15, 20][1,5,10,15,20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "To perform Min-Max scaling to transform the values to a range of -1 to 1, we use the formula:\n",
    "Xscaled=(X−Xmin)(Xmax−Xmin)×(1−(−1))+(−1)X_{\\text{scaled}} = \\frac{(X - X_{\\text{min}})}{(X_{\\text{max}} - X_{\\text{min}})} \\times (1 - (-1)) + (-1)Xscaled=(Xmax−Xmin)(X−Xmin)×(1−(−1))+(−1)\n",
    "Given the values [1,5,10,15,20][1, 5, 10, 15, 20][1,5,10,15,20], Xmin=1X_{\\text{min}} = 1Xmin=1 and Xmax=20X_{\\text{max}} = 20Xmax=20:\n",
    "For X=1X = 1X=1:\n",
    "Xscaled=(1−1)(20−1)×2+(−1)=−1X_{\\text{scaled}} = \\frac{(1 - 1)}{(20 - 1)} \\times 2 + (-1) = -1Xscaled=(20−1)(1−1)×2+(−1)=−1\n",
    "For X=5X = 5X=5:\n",
    "Xscaled=(5−1)(20−1)×2+(−1)=−0.7895X_{\\text{scaled}} = \\frac{(5 - 1)}{(20 - 1)} \\times 2 + (-1) = -0.7895Xscaled=(20−1)(5−1)×2+(−1)=−0.7895\n",
    "For X=10X = 10X=10:\n",
    "Xscaled=(10−1)(20−1)×2+(−1)=−0.5789X_{\\text{scaled}} = \\frac{(10 - 1)}{(20 - 1)} \\times 2 + (-1) = -0.5789Xscaled=(20−1)(10−1)×2+(−1)=−0.5789\n",
    "For X=15X = 15X=15:\n",
    "Xscaled=(15−1)(20−1)×2+(−1)=0.1579X_{\\text{scaled}} = \\frac{(15 - 1)}{(20 - 1)} \\times 2 + (-1) = 0.1579Xscaled=(20−1)(15−1)×2+(−1)=0.1579\n",
    "For X=20X = 20X=20:\n",
    "Xscaled=(20−1)(20−1)×2+(−1)=1X_{\\text{scaled}} = \\frac{(20 - 1)}{(20 - 1)} \\times 2 + (-1) = 1Xscaled=(20−1)(20−1)×2+(−1)=1\n",
    "The scaled values are: [−1,−0.7895,−0.5789,0.1579,1][-1, -0.7895, -0.5789, 0.1579, 1][−1,−0.7895,−0.5789,0.1579,1].\n",
    "________________________________________\n",
    "Q8: For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "To perform Feature Extraction using PCA on a dataset with features [height, weight, age, gender, blood pressure]:\n",
    "1.\tPreprocessing:\n",
    "o\tConvert categorical features (e.g., gender) into numerical format.\n",
    "o\tStandardize the dataset to ensure all features have zero mean and unit variance.\n",
    "1.\tApply PCA: Compute the principal components by finding the eigenvectors of the covariance matrix of the dataset.\n",
    "2.\tRetain principal components:\n",
    "o\tAfter applying PCA, examine the eigenvalues (variance explained by each principal component).\n",
    "o\tChoose the number of components that explain a sufficient amount of variance (e.g., 95%).\n",
    "In this case, it’s likely that 3 components will capture most of the variance:\n",
    "•\tHeight and weight are often correlated and might be summarized into a single component.\n",
    "•\tAge and blood pressure may be another significant component.\n",
    "•\tGender may have minimal variance and might not contribute much, but still, PCA will identify the best components.\n",
    "Retaining 3 components would reduce dimensionality while preserving the most important information in the dataset.\n",
    "2.\t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
